#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒè½¨åˆ¶æ–‡æ¡£æ„å»ºæ¨¡å—
- å¯Œæ–‡æœ¬ç´¢å¼•ï¼šæœ‰ LLM æ‘˜è¦çš„ 1100 éƒ¨å‰§é›†ï¼ˆä½¿ç”¨ LLM_sum.jsonï¼‰
- åŸºç¡€ç´¢å¼•ï¼šå…¶ä»–å‰§é›†ï¼ˆä½¿ç”¨åŸå§‹ summaryï¼‰

æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ï¼š
1. å¯Œæ–‡æœ¬ç´¢å¼•ä½¿ç”¨ plot_summaryï¼ˆ500å­—ç²¾ç‚¼æ‘˜è¦ï¼‰è€Œé combined_textï¼ˆé¿å…é‡å¤å’Œæ··æ·†ï¼‰
2. æ ‡ç­¾ä½œä¸º metadata ç”¨äºè¿‡æ»¤å’Œå¢å¼ºæ£€ç´¢
3. äººç‰©ä¸çœ‹ç‚¹çš„å…³é”®ä¿¡æ¯æå–ä¸ºç»“æ„åŒ– metadata
"""

import sqlite3
import json
import os
import logging
import re
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple
from functools import lru_cache

# å°è¯•å¯¼å…¥ tqdmï¼ˆå¦‚æœå¯ç”¨ï¼Œç”¨äºè¿›åº¦æ¡ï¼‰
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # å¦‚æœæ²¡æœ‰ tqdmï¼Œä½¿ç”¨ç®€å•çš„åŒ…è£…å™¨
    def tqdm(iterable, desc="", unit="", total=None):
        return iterable

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
COMPILED_PATTERNS = {
    'occupation': [
        re.compile(r'èŒä¸šèº«ä»½[ï¼š:]\s*([^\n*]+)'),
        re.compile(r'èŒä¸šèº«ä»½[ï¼š:]\s*([^\n]+?)(?:\n|$)'),
        re.compile(r'èŒä¸š[ï¼š:]\s*([^\n*]+)'),
    ],
    'trait': [
        re.compile(r'æ€§æ ¼ç‰¹å¾[ï¼š:]\s*([^\n*]+)'),
        re.compile(r'æ€§æ ¼[ï¼š:]\s*([^\n*]+)'),
        re.compile(r'æ€§æ ¼ç‰¹å¾[ï¼š:]\s*([^\n]+?)(?:\n|$)'),
    ],
    'relationship': [
        re.compile(r'([^\n]+?)[ä¸å’Œ][^\n]+?[ï¼š:]\s*([^\n]+)'),
        re.compile(r'([A-Za-z0-9\u4e00-\u9fa5]+)\s*ä¸\s*([A-Za-z0-9\u4e00-\u9fa5]+)[ï¼š:]\s*([^\n]+)'),
    ],
    'conflict': [
        re.compile(r'æ ¸å¿ƒå†²çª[^ï¼š:]*[ï¼š:]\s*\n\s*[-â€¢Â·]\s*([^\n]+)'),
        re.compile(r'æ ¸å¿ƒå†²çª[^ï¼š:]*[ï¼š:]\s*([^\n]+)'),
        re.compile(r'å†²çª[^ï¼š:]*[ï¼š:]\s*\n\s*[-â€¢Â·]\s*([^\n]+)'),
    ]
}

# é…ç½®ç¯å¢ƒå˜é‡
if 'HF_ENDPOINT' not in os.environ:
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['SAFETENSORS_FAST_GPU'] = '1'
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('ALL_PROXY', None)
os.environ.pop('all_proxy', None)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

from llama_index.core import Document
from llama_index.core.schema import RelatedNodeInfo, NodeRelationship
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings

# é…ç½® Embedding
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-zh-v1.5")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/dual_track.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_llm_summaries(llm_sum_file: str = "data/llm_summaries.json") -> Dict:
    """
    åŠ è½½ LLM ç”Ÿæˆçš„æ‘˜è¦ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†å’Œæ–‡ä»¶å¤§å°æ£€æŸ¥ï¼‰
    
    Returns:
        dict: {series_id: summary_data}
    """
    if not Path(llm_sum_file).exists():
        logger.warning(f"{llm_sum_file} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åŸºç¡€ç´¢å¼•")
        print(f"âš  è­¦å‘Š: {llm_sum_file} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åŸºç¡€ç´¢å¼•")
        return {}
    
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = Path(llm_sum_file).stat().st_size / (1024 * 1024)  # MB
        if file_size > 500:  # è¶…è¿‡ 500MB è­¦å‘Š
            logger.warning(f"llm_summaries.json æ–‡ä»¶è¾ƒå¤§ ({file_size:.1f}MB)ï¼ŒåŠ è½½å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")
        
        with open(llm_sum_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if not isinstance(data, dict):
            logger.error(f"llm_summaries.json æ ¼å¼é”™è¯¯ï¼šæœŸæœ› dictï¼Œå¾—åˆ° {type(data)}")
            raise ValueError(f"llm_summaries.json æ ¼å¼é”™è¯¯ï¼šæœŸæœ› dictï¼Œå¾—åˆ° {type(data)}")
        
        logger.info(f"æˆåŠŸåŠ è½½ LLM æ‘˜è¦: {len(data)} ä¸ªæ¡ç›®")
        print(f"âœ“ åŠ è½½ LLM æ‘˜è¦: {len(data)} ä¸ªæ¡ç›®")
        return data
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON è§£æå¤±è´¥: {e}")
        print(f"âŒ é”™è¯¯: JSON è§£æå¤±è´¥: {e}")
        return {}
    except Exception as e:
        logger.error(f"åŠ è½½ LLM æ‘˜è¦å¤±è´¥: {e}", exc_info=True)
        print(f"âŒ é”™è¯¯: åŠ è½½ LLM æ‘˜è¦å¤±è´¥: {e}")
        return {}


def extract_character_profile(combined_text: str) -> str:
    """
    ä» combined_text ä¸­æå–"äººç‰©ä¸çœ‹ç‚¹"éƒ¨åˆ†ï¼ˆå»æ‰é‡å¤çš„å‰§æƒ…æ¢—æ¦‚ï¼‰
    
    Returns:
        str: äººç‰©ä¸çœ‹ç‚¹çš„å®Œæ•´æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not combined_text or "=== äººç‰©ä¸çœ‹ç‚¹ ===" not in combined_text:
        return ""
    
    try:
        # æå–äººç‰©ä¸çœ‹ç‚¹éƒ¨åˆ†ï¼ˆå»æ‰"å‰§æƒ…æ¢—æ¦‚"éƒ¨åˆ†ï¼‰
        parts = combined_text.split("=== äººç‰©ä¸çœ‹ç‚¹ ===")
        if len(parts) < 2:
            return ""
        
        character_section = parts[1].split("=== å‰§æƒ…æ¢—æ¦‚ ===")[0].strip()
        return character_section
    except Exception as e:
        logger.warning(f"æå–äººç‰©ä¸çœ‹ç‚¹å¤±è´¥: {e}", exc_info=True)
        print(f"  âš  æå–äººç‰©ä¸çœ‹ç‚¹å¤±è´¥: {e}")
        return ""


def extract_character_keywords(combined_text: str) -> Dict[str, List[str]]:
    """
    ä» combined_text çš„"äººç‰©ä¸çœ‹ç‚¹"éƒ¨åˆ†æå–å…³é”®ä¿¡æ¯ï¼ˆç”¨äº metadataï¼‰
    
    æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ›´å¥å£®ï¼š
    - æ”¯æŒä¸­è‹±æ–‡å†’å·ã€å…¨è§’åŠè§’
    - æ”¯æŒå¤šç§ç©ºæ ¼å’Œæ¢è¡Œæ ¼å¼
    - å®¹é”™æ€§æ›´å¼º
    
    Returns:
        dict: {
            'occupations': [èŒä¸šåˆ—è¡¨],
            'character_traits': [æ€§æ ¼ç‰¹å¾åˆ—è¡¨],
            'relationships': [å…³ç³»æ¨¡å¼åˆ—è¡¨],
            'conflicts': [æ ¸å¿ƒå†²çªåˆ—è¡¨]
        }
    """
    result = {
        'occupations': [],
        'character_traits': [],
        'relationships': [],
        'conflicts': []
    }
    
    if not combined_text or "=== äººç‰©ä¸çœ‹ç‚¹ ===" not in combined_text:
        return result
    
    try:
        # æå–äººç‰©ä¸çœ‹ç‚¹éƒ¨åˆ†
        parts = combined_text.split("=== äººç‰©ä¸çœ‹ç‚¹ ===")
        if len(parts) < 2:
            return result
        
        character_section = parts[1].split("=== å‰§æƒ…æ¢—æ¦‚ ===")[0]
        
        # ä½¿ç”¨é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        # æ”¹è¿›çš„èŒä¸šèº«ä»½æå–ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        for pattern in COMPILED_PATTERNS['occupation']:
            occupations = pattern.findall(character_section)
            if occupations:
                result['occupations'] = [o.strip() for o in occupations if o.strip()]
                break
        
        # æ”¹è¿›çš„æ€§æ ¼ç‰¹å¾æå–
        for pattern in COMPILED_PATTERNS['trait']:
            traits = pattern.findall(character_section)
            if traits:
                result['character_traits'] = [t.strip() for t in traits if t.strip()]
                break
        
        # æ”¹è¿›çš„å…³ç³»æ¨¡å¼æå–ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
        for pattern in COMPILED_PATTERNS['relationship']:
            relationships = pattern.findall(character_section)
            if relationships:
                # å¤„ç†ä¸åŒçš„åŒ¹é…ç»„æ ¼å¼
                if isinstance(relationships[0], tuple):
                    if len(relationships[0]) == 3:
                        result['relationships'] = [f"{r[0]}ä¸{r[1]}ï¼š{r[2]}" for r in relationships]
                    else:
                        result['relationships'] = [r.strip() if isinstance(r, str) else ' '.join(r).strip() for r in relationships]
                else:
                    result['relationships'] = [r.strip() for r in relationships if r.strip()]
                break
        
        # æ”¹è¿›çš„æ ¸å¿ƒå†²çªæå–ï¼ˆæ”¯æŒå¤šç§åˆ—è¡¨æ ¼å¼ï¼‰
        for pattern in COMPILED_PATTERNS['conflict']:
            conflicts = pattern.findall(character_section)
            if conflicts:
                result['conflicts'] = [c.strip() for c in conflicts if c.strip()]
                break
        
    except Exception as e:
        print(f"  âš  æå–äººç‰©å…³é”®è¯å¤±è´¥: {e}")
    
    return result


def load_series_with_episodes_generator(db_path: str):
    """
    ä»SQLiteæ•°æ®åº“æµå¼åŠ è½½å‰§é›†åŠå…¶åˆ†é›†æ•°æ®ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
    
    ä¿®å¤ï¼šç¡®ä¿æ•°æ®åº“è¿æ¥åœ¨ finally å—ä¸­å…³é—­ï¼Œé¿å…è¿æ¥æ³„æ¼
    
    Yields:
        Dict: å•ä¸ªå‰§é›†æ•°æ®ï¼ˆåŒ…å«episodesåˆ—è¡¨ï¼‰
    """
    conn = None
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # ä½¿ç”¨è¿­ä»£å™¨æ¨¡å¼ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®
        cursor.execute('''
            SELECT 
                s.id as series_id,
                s.original_doc_id,
                s.title,
                s.summary,
                s.cast,
                s.director,
                s.year,
                s.region,
                s.genre,
                s.url as series_url,
                e.id as episode_id,
                e.ep_number,
                e.episode_title,
                e.content as episode_content,
                e.episode_url
            FROM series s
            LEFT JOIN episodes e ON s.id = e.series_id
            WHERE s.title IS NOT NULL
            ORDER BY s.id, e.ep_number
        ''')
        
        current_series = None
        current_series_id = None
        
        # æµå¼å¤„ç†ï¼Œé€è¡Œè¯»å–
        for row in cursor:
            try:
                series_id = row['series_id']
                
                # å¦‚æœé‡åˆ°æ–°çš„å‰§é›†ï¼Œå…ˆ yield ä¸Šä¸€ä¸ªå‰§é›†
                if current_series_id is not None and series_id != current_series_id:
                    yield current_series
                    current_series = None
                
                # åˆå§‹åŒ–æ–°å‰§é›†
                if current_series is None:
                    current_series = {
                        'series_id': series_id,
                        'original_doc_id': row['original_doc_id'],
                        'title': row['title'],
                        'summary': row['summary'] or '',
                        'cast': row['cast'] or '',
                        'director': row['director'] or '',
                        'year': row['year'] or '',
                        'region': row['region'] or '',
                        'genre': row['genre'] or '',
                        'url': row['series_url'],
                        'episodes': []
                    }
                    current_series_id = series_id
                
                # æ·»åŠ åˆ†é›†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if row['episode_id']:
                    current_series['episodes'].append({
                        'ep_number': row['ep_number'],
                        'episode_title': row['episode_title'] or '',
                        'content': row['episode_content'] or '',
                        'episode_url': row['episode_url'] or ''
                    })
            except Exception as e:
                logger.error(f"å¤„ç†æ•°æ®åº“è¡Œæ—¶å‡ºé”™: {e}", exc_info=True)
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œï¼Œç»§ç»­å¤„ç†
        
        # yield æœ€åä¸€ä¸ªå‰§é›†
        if current_series is not None:
            yield current_series
            
    except sqlite3.Error as e:
        logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise
    finally:
        # ç¡®ä¿æ•°æ®åº“è¿æ¥å…³é—­
        if conn is not None:
            try:
                conn.close()
                logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}", exc_info=True)


def build_rich_text_document(
    series: Dict,
    llm_summary: Dict
) -> Tuple[Document, List[Document]]:
    """
    æ„å»ºå¯Œæ–‡æœ¬ç´¢å¼•çš„æ–‡æ¡£ï¼ˆæœ‰ LLM æ‘˜è¦çš„å‰§é›†ï¼‰
    
    ç­–ç•¥ï¼š
    - ä½¿ç”¨ plot_summary + äººç‰©ä¸çœ‹ç‚¹ï¼ˆå»æ‰é‡å¤éƒ¨åˆ†ï¼‰ä½œä¸ºä¸»è¦æ–‡æœ¬
    - äººç‰©ä¸çœ‹ç‚¹çš„å®Œæ•´å†…å®¹æ¯” tags æ›´ç²¾å‡†ï¼Œè¦å……åˆ†ä½¿ç”¨
    - æ ‡ç­¾ä½œä¸º metadataï¼ˆç”¨äºè¿‡æ»¤ï¼‰
    - äººç‰©ä¸çœ‹ç‚¹çš„å…³é”®ä¿¡æ¯ä¹Ÿæå–ä¸º metadataï¼ˆåŒé‡åˆ©ç”¨ï¼‰
    
    Returns:
        tuple: (parent_doc, child_docs)
    """
    series_id = series['series_id']
    
    # ä½¿ç”¨ plot_summary ä½œä¸ºå‰§æƒ…æ‘˜è¦
    plot_summary = llm_summary.get('plot_summary', series.get('summary', '') or 'æš‚æ— å‰§æƒ…ç®€ä»‹')
    
    # ä» combined_text æå–äººç‰©ä¸çœ‹ç‚¹éƒ¨åˆ†ï¼ˆå»æ‰é‡å¤çš„å‰§æƒ…æ¢—æ¦‚ï¼‰
    combined_text = llm_summary.get('combined_text', '')
    character_profile = extract_character_profile(combined_text)
    
    # æ„å»ºçˆ¶èŠ‚ç‚¹æ–‡æœ¬ï¼ˆå……åˆ†åˆ©ç”¨äººç‰©ä¸çœ‹ç‚¹ä¿¡æ¯ï¼‰
    parent_content_parts = []
    parent_content_parts.append(f"å‰§åï¼š{series['title']}")
    
    if series['cast']:
        parent_content_parts.append(f"ä¸»æ¼”ï¼š{series['cast']}")
    if series['director']:
        parent_content_parts.append(f"å¯¼æ¼”ï¼š{series['director']}")
    if series['year']:
        parent_content_parts.append(f"å¹´ä»½ï¼š{series['year']}")
    if series['region']:
        parent_content_parts.append(f"åœ°åŒºï¼š{series['region']}")
    if series['genre']:
        parent_content_parts.append(f"ç±»å‹ï¼š{series['genre']}")
    
    # æ·»åŠ äººç‰©ä¸çœ‹ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯æ¯” tags æ›´ç²¾å‡†ï¼‰
    if character_profile:
        parent_content_parts.append(f"\näººç‰©ä¸çœ‹ç‚¹ï¼š\n{character_profile}")
    
    # æ·»åŠ å‰§æƒ…æ‘˜è¦
    parent_content_parts.append(f"\nå‰§æƒ…æ‘˜è¦ï¼š\n{plot_summary}")
    parent_text = "\n".join(parent_content_parts)
    
    # æ„å»º metadataï¼ˆæœ€å¤§åŒ–åˆ©ç”¨ LLM ç”Ÿæˆçš„ä¿¡æ¯ï¼‰
    doc_metadata = {
        "series_id": series_id,
        "doc_id": series['original_doc_id'],
        "title": series['title'],
        "year": series['year'] or '',
        "region": series['region'] or '',
        "genre": series['genre'] or '',
        "url": series['url'],
        "type": "series",
        "index_type": "rich_text",  # æ ‡è®°ä¸ºå¯Œæ–‡æœ¬ç´¢å¼•
        "episode_count": len(series['episodes']),
        "has_llm_summary": True
    }
    
    # æ·»åŠ æ ‡ç­¾åˆ° metadataï¼ˆç”¨äºè¿‡æ»¤å’Œå¢å¼ºæ£€ç´¢ï¼‰
    if llm_summary.get('tags'):
        doc_metadata['tags'] = ','.join(llm_summary['tags'])
    if llm_summary.get('occupation_tags'):
        doc_metadata['occupation_tags'] = ','.join(llm_summary['occupation_tags'])
    if llm_summary.get('character_tags'):
        doc_metadata['character_tags'] = ','.join(llm_summary['character_tags'])
    if llm_summary.get('style_tags'):
        doc_metadata['style_tags'] = ','.join(llm_summary['style_tags'])
    
    # ä» combined_text æå–äººç‰©ä¸çœ‹ç‚¹çš„å…³é”®ä¿¡æ¯ï¼ˆç”¨äº metadata è¿‡æ»¤ï¼‰
    # æ³¨æ„ï¼šäººç‰©ä¸çœ‹ç‚¹çš„å®Œæ•´æ–‡æœ¬å·²ç»åŠ å…¥åˆ°ä¸»æ–‡æœ¬ä¸­ï¼Œè¿™é‡Œæå–çš„æ˜¯ç»“æ„åŒ–å…³é”®è¯ç”¨äºè¿‡æ»¤
    if combined_text:
        char_keywords = extract_character_keywords(combined_text)
        if char_keywords['occupations']:
            doc_metadata['extracted_occupations'] = ','.join(char_keywords['occupations'][:5])  # é™åˆ¶æ•°é‡
        if char_keywords['character_traits']:
            doc_metadata['extracted_traits'] = ','.join(char_keywords['character_traits'][:5])
        if char_keywords['relationships']:
            doc_metadata['extracted_relationships'] = ','.join(char_keywords['relationships'][:3])
        if char_keywords['conflicts']:
            doc_metadata['extracted_conflicts'] = ','.join(char_keywords['conflicts'][:3])
    
    # ä¸ºçˆ¶æ–‡æ¡£ç”Ÿæˆå”¯ä¸€çš„ node_id
    parent_node_id = f"series_{series_id}_{series['original_doc_id']}"
    
    parent_doc = Document(
        text=parent_text,
        metadata=doc_metadata,
        excluded_embed_metadata_keys=["series_id", "doc_id", "url", "index_type", "has_llm_summary"],
        id_=parent_node_id  # è®¾ç½®çˆ¶æ–‡æ¡£çš„ node_id
    )
    
    # æ„å»ºå­èŠ‚ç‚¹ï¼ˆåˆ†é›†ï¼‰- å……åˆ†åˆ©ç”¨åˆ†é›†å‰§æƒ…ï¼Œå¹¶å»ºç«‹çˆ¶å­å…³ç³»
    child_docs = []
    episodes_with_content = 0
    for ep in series['episodes']:
        if not ep.get('content'):
            continue
        
        episodes_with_content += 1
        
        # æ„å»ºåˆ†é›†æ–‡æœ¬ï¼ˆå……åˆ†åˆ©ç”¨åˆ†é›†å†…å®¹ï¼‰
        ep_text_parts = []
        ep_text_parts.append(f"ç¬¬{ep['ep_number']}é›†")
        if ep.get('episode_title'):
            ep_text_parts.append(f"ã€Š{ep['episode_title']}ã€‹")
        ep_text_parts.append(f"\n{ep['content']}")
        
        ep_text = "".join(ep_text_parts)
        
        # ä¸ºå­æ–‡æ¡£ç”Ÿæˆå”¯ä¸€çš„ node_id
        child_node_id = f"episode_{series_id}_{ep['ep_number']}"
        
        # å»ºç«‹çˆ¶å­å…³ç³»ï¼šå­æ–‡æ¡£æŒ‡å‘çˆ¶æ–‡æ¡£
        relationships = {
            NodeRelationship.PARENT: RelatedNodeInfo(
                node_id=parent_node_id,
                node_type="series",
                metadata={"title": series['title']}
            )
        }
        
        # æ¯ä¸ªåˆ†é›†ä½œä¸ºç‹¬ç«‹çš„ Documentï¼Œä¼šè¢«å•ç‹¬ç´¢å¼•å’Œæ£€ç´¢ï¼Œä½†å»ºç«‹äº†çˆ¶å­å…³ç³»
        child_doc = Document(
            text=ep_text,
            metadata={
                "series_id": series_id,
                "ep_number": ep['ep_number'],
                "episode_title": ep['episode_title'] or '',
                "episode_url": ep['episode_url'],
                "type": "episode",
                "parent_title": series['title'],
                "parent_doc_id": series['original_doc_id'],
                "index_type": "rich_text"
            },
            excluded_embed_metadata_keys=["series_id", "episode_url", "parent_doc_id", "index_type"],
            id_=child_node_id,
            relationships=relationships  # å»ºç«‹çˆ¶å­å…³ç³»
        )
        child_docs.append(child_doc)
    
    return parent_doc, child_docs


def build_basic_document(series: Dict) -> Tuple[Document, List[Document]]:
    """
    æ„å»ºåŸºç¡€ç´¢å¼•çš„æ–‡æ¡£ï¼ˆæ²¡æœ‰ LLM æ‘˜è¦çš„å‰§é›†ï¼‰
    
    Returns:
        tuple: (parent_doc, child_docs)
    """
    series_id = series['series_id']
    
    # ä½¿ç”¨åŸå§‹ summary
    summary = series.get('summary', '') or 'æš‚æ— å‰§æƒ…ç®€ä»‹'
    
    # æ„å»ºçˆ¶èŠ‚ç‚¹æ–‡æœ¬
    parent_content_parts = []
    parent_content_parts.append(f"å‰§åï¼š{series['title']}")
    
    if series['cast']:
        parent_content_parts.append(f"ä¸»æ¼”ï¼š{series['cast']}")
    if series['director']:
        parent_content_parts.append(f"å¯¼æ¼”ï¼š{series['director']}")
    if series['year']:
        parent_content_parts.append(f"å¹´ä»½ï¼š{series['year']}")
    if series['region']:
        parent_content_parts.append(f"åœ°åŒºï¼š{series['region']}")
    if series['genre']:
        parent_content_parts.append(f"ç±»å‹ï¼š{series['genre']}")
    
    parent_content_parts.append(f"\nå‰§æƒ…æ‘˜è¦ï¼š\n{summary}")
    parent_text = "\n".join(parent_content_parts)
    
    # æ„å»º metadata
    doc_metadata = {
        "series_id": series_id,
        "doc_id": series['original_doc_id'],
        "title": series['title'],
        "year": series['year'] or '',
        "region": series['region'] or '',
        "genre": series['genre'] or '',
        "url": series['url'],
        "type": "series",
        "index_type": "basic",  # æ ‡è®°ä¸ºåŸºç¡€ç´¢å¼•
        "episode_count": len(series['episodes']),
        "has_llm_summary": False
    }
    
    # ä¸ºçˆ¶æ–‡æ¡£ç”Ÿæˆå”¯ä¸€çš„ node_id
    parent_node_id = f"series_{series_id}_{series['original_doc_id']}"
    
    parent_doc = Document(
        text=parent_text,
        metadata=doc_metadata,
        excluded_embed_metadata_keys=["series_id", "doc_id", "url", "index_type", "has_llm_summary"],
        id_=parent_node_id  # è®¾ç½®çˆ¶æ–‡æ¡£çš„ node_id
    )
    
    # æ„å»ºå­èŠ‚ç‚¹ï¼ˆåˆ†é›†ï¼‰- å……åˆ†åˆ©ç”¨åˆ†é›†å‰§æƒ…ï¼Œå¹¶å»ºç«‹çˆ¶å­å…³ç³»
    child_docs = []
    for ep in series['episodes']:
        if not ep.get('content'):
            continue
        
        # æ„å»ºåˆ†é›†æ–‡æœ¬ï¼ˆå……åˆ†åˆ©ç”¨åˆ†é›†å†…å®¹ï¼‰
        ep_text_parts = []
        ep_text_parts.append(f"ç¬¬{ep['ep_number']}é›†")
        if ep.get('episode_title'):
            ep_text_parts.append(f"ã€Š{ep['episode_title']}ã€‹")
        ep_text_parts.append(f"\n{ep['content']}")
        
        ep_text = "".join(ep_text_parts)
        
        # ä¸ºå­æ–‡æ¡£ç”Ÿæˆå”¯ä¸€çš„ node_id
        child_node_id = f"episode_{series_id}_{ep['ep_number']}"
        
        # å»ºç«‹çˆ¶å­å…³ç³»ï¼šå­æ–‡æ¡£æŒ‡å‘çˆ¶æ–‡æ¡£
        relationships = {
            NodeRelationship.PARENT: RelatedNodeInfo(
                node_id=parent_node_id,
                node_type="series",
                metadata={"title": series['title']}
            )
        }
        
        # æ¯ä¸ªåˆ†é›†ä½œä¸ºç‹¬ç«‹çš„ Documentï¼Œä¼šè¢«å•ç‹¬ç´¢å¼•å’Œæ£€ç´¢ï¼Œä½†å»ºç«‹äº†çˆ¶å­å…³ç³»
        child_doc = Document(
            text=ep_text,
            metadata={
                "series_id": series_id,
                "ep_number": ep['ep_number'],
                "episode_title": ep['episode_title'] or '',
                "episode_url": ep['episode_url'],
                "type": "episode",
                "parent_title": series['title'],
                "parent_doc_id": series['original_doc_id'],
                "index_type": "basic"
            },
            excluded_embed_metadata_keys=["series_id", "episode_url", "parent_doc_id", "index_type"],
            id_=child_node_id,
            relationships=relationships  # å»ºç«‹çˆ¶å­å…³ç³»
        )
        child_docs.append(child_doc)
    
    return parent_doc, child_docs


def build_dual_track_documents_generator(
    db_path: str = "data/database/final.db",
    llm_sum_file: str = "data/llm_summaries.json",
    filter_type: Optional[str] = None
):
    """
    æ„å»ºåŒè½¨åˆ¶æ–‡æ¡£ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
    
    ä¼˜åŒ–ï¼šä¸€æ¬¡éå†æ•°æ®åº“ï¼ŒåŒæ—¶ç”Ÿæˆä¸¤ç§ç±»å‹çš„æ–‡æ¡£ï¼Œé¿å…é‡å¤åŠ è½½
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„
        llm_sum_file: LLMæ‘˜è¦æ–‡ä»¶è·¯å¾„
        filter_type: è¿‡æ»¤ç±»å‹ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤ï¼Œ"rich_text"æˆ–"basic"è¡¨ç¤ºåªç”ŸæˆæŒ‡å®šç±»å‹
    
    Yields:
        Document: Document å¯¹è±¡ï¼ˆå¦‚æœ filter_type æŒ‡å®šï¼Œåªè¿”å›è¯¥ç±»å‹ï¼‰
    """
    print("="*60)
    print("åŒè½¨åˆ¶æ–‡æ¡£æ„å»ºæ¨¡å—ï¼ˆæµå¼å¤„ç†ï¼‰")
    print("="*60)
    
    # 1. åŠ è½½ LLM æ‘˜è¦
    print("\næ­£åœ¨åŠ è½½ LLM æ‘˜è¦...")
    llm_summaries = load_llm_summaries(llm_sum_file)
    llm_series_ids = {int(k) for k in llm_summaries.keys()}
    print(f"  âœ“ æœ‰ LLM æ‘˜è¦çš„å‰§é›†: {len(llm_series_ids)} éƒ¨")
    
    # 2. æµå¼å¤„ç†æ•°æ®åº“æ•°æ®ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼‰
    print("\næ­£åœ¨æµå¼å¤„ç†æ•°æ®åº“æ•°æ®...")
    series_generator = load_series_with_episodes_generator(db_path)
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨åˆ—è¡¨å­˜å‚¨ï¼Œå› ä¸ºç”Ÿæˆå™¨åªèƒ½è¿­ä»£ä¸€æ¬¡ï¼‰
    stats = {
        'rich_text_count': 0,
        'basic_count': 0,
        'rich_text_episodes': 0,
        'basic_episodes': 0,
        'extraction_stats': {
            'character_profile_success': 0,
            'character_profile_failed': 0,
            'keywords_success': 0,
            'keywords_failed': 0
        }
    }
    
    print("\næ­£åœ¨æ„å»ºæ–‡æ¡£ï¼ˆæµå¼å¤„ç†ï¼‰...")
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if HAS_TQDM:
        series_generator = tqdm(series_generator, desc="å¤„ç†å‰§é›†", unit="éƒ¨")
    
    for idx, series in enumerate(series_generator, 1):
        if not HAS_TQDM and idx % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {idx} éƒ¨å‰§é›†...")
        
        series_id = series['series_id']
        
        if series_id in llm_series_ids:
            # å¯Œæ–‡æœ¬ç´¢å¼•ï¼ˆä¿®å¤ï¼šä½¿ç”¨ .get() æ–¹æ³•ï¼Œæ·»åŠ ç©ºå€¼æ£€æŸ¥ï¼‰
            llm_summary = llm_summaries.get(str(series_id))
            if not llm_summary:
                logger.warning(f"series_id {series_id} åœ¨ llm_series_ids ä¸­ä½† llm_summaries ä¸­ä¸å­˜åœ¨ï¼Œé™çº§åˆ°åŸºç¡€ç´¢å¼•")
                # é™çº§åˆ°åŸºç¡€ç´¢å¼•
                parent_doc, child_docs = build_basic_document(series)
                if filter_type is None or filter_type == "basic":
                    yield parent_doc
                stats['basic_count'] += 1
                for child_doc in child_docs:
                    if filter_type is None or filter_type == "basic":
                        yield child_doc
                    stats['basic_episodes'] += 1
                continue
            
            try:
                # ç»Ÿè®¡äººç‰©ä¸çœ‹ç‚¹æå–æˆåŠŸç‡
                combined_text = llm_summary.get('combined_text', '')
                character_profile = extract_character_profile(combined_text)
                if character_profile:
                    stats['extraction_stats']['character_profile_success'] += 1
                else:
                    stats['extraction_stats']['character_profile_failed'] += 1
                
                # ç»Ÿè®¡å…³é”®è¯æå–æˆåŠŸç‡
                char_keywords = extract_character_keywords(combined_text)
                if any(char_keywords.values()):
                    stats['extraction_stats']['keywords_success'] += 1
                else:
                    stats['extraction_stats']['keywords_failed'] += 1
                
                parent_doc, child_docs = build_rich_text_document(series, llm_summary)
            except Exception as e:
                logger.error(f"æ„å»ºå¯Œæ–‡æœ¬æ–‡æ¡£å¤±è´¥ (series_id={series_id}): {e}", exc_info=True)
                # é™çº§åˆ°åŸºç¡€ç´¢å¼•
                parent_doc, child_docs = build_basic_document(series)
                if filter_type is None or filter_type == "basic":
                    yield parent_doc
                stats['basic_count'] += 1
                for child_doc in child_docs:
                    if filter_type is None or filter_type == "basic":
                        yield child_doc
                    stats['basic_episodes'] += 1
                continue
            
            # yield çˆ¶èŠ‚ç‚¹ï¼ˆå¦‚æœä¸è¿‡æ»¤æˆ–è¿‡æ»¤ç±»å‹åŒ¹é…ï¼‰
            if filter_type is None or filter_type == "rich_text":
                yield parent_doc
            stats['rich_text_count'] += 1
            
            # yield å­èŠ‚ç‚¹ï¼ˆåˆ†é›†ï¼‰
            for child_doc in child_docs:
                if filter_type is None or filter_type == "rich_text":
                    yield child_doc
                stats['rich_text_episodes'] += 1
        else:
            # åŸºç¡€ç´¢å¼•
            parent_doc, child_docs = build_basic_document(series)
            
            # yield çˆ¶èŠ‚ç‚¹ï¼ˆå¦‚æœä¸è¿‡æ»¤æˆ–è¿‡æ»¤ç±»å‹åŒ¹é…ï¼‰
            if filter_type is None or filter_type == "basic":
                yield parent_doc
            stats['basic_count'] += 1
            
            # yield å­èŠ‚ç‚¹ï¼ˆåˆ†é›†ï¼‰
            for child_doc in child_docs:
                if filter_type is None or filter_type == "basic":
                    yield child_doc
                stats['basic_episodes'] += 1
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ï¼ˆåªåœ¨ä¸è¿‡æ»¤æ—¶è¾“å‡ºï¼Œé¿å…é‡å¤ï¼‰
    if filter_type is None:
        print(f"\nâœ“ æ–‡æ¡£æ„å»ºå®Œæˆ:")
        print(f"  - å¯Œæ–‡æœ¬ç´¢å¼•: {stats['rich_text_count']} éƒ¨å‰§é›†, {stats['rich_text_episodes']} ä¸ªåˆ†é›†")
        print(f"  - åŸºç¡€ç´¢å¼•: {stats['basic_count']} éƒ¨å‰§é›†, {stats['basic_episodes']} ä¸ªåˆ†é›†")
        print(f"  - æ€»è®¡: {stats['rich_text_count'] + stats['basic_count']} éƒ¨å‰§é›†, {stats['rich_text_episodes'] + stats['basic_episodes']} ä¸ªåˆ†é›†")
        
        # è¾“å‡ºæå–ç»Ÿè®¡
        total_rich = stats['rich_text_count']
        if total_rich > 0:
            profile_success_rate = stats['extraction_stats']['character_profile_success'] / total_rich * 100
            keywords_success_rate = stats['extraction_stats']['keywords_success'] / total_rich * 100
            print(f"\nğŸ“Š æå–ç»Ÿè®¡ï¼ˆå¯Œæ–‡æœ¬ç´¢å¼•ï¼‰:")
            print(f"  - äººç‰©ä¸çœ‹ç‚¹æå–æˆåŠŸç‡: {profile_success_rate:.1f}% ({stats['extraction_stats']['character_profile_success']}/{total_rich})")
            print(f"  - å…³é”®è¯æå–æˆåŠŸç‡: {keywords_success_rate:.1f}% ({stats['extraction_stats']['keywords_success']}/{total_rich})")
            if stats['extraction_stats']['character_profile_failed'] > 0:
                print(f"  âš  è­¦å‘Š: {stats['extraction_stats']['character_profile_failed']} ä¸ªæ¡ç›®çš„äººç‰©ä¸çœ‹ç‚¹æå–å¤±è´¥")


def build_dual_track_documents(
    db_path: str = "data/database/final.db",
    llm_sum_file: str = "data/llm_summaries.json"
) -> Tuple[List[Document], List[Document]]:
    """
    æ„å»ºåŒè½¨åˆ¶æ–‡æ¡£ï¼ˆå…¼å®¹æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¼šåŠ è½½æ‰€æœ‰æ–‡æ¡£åˆ°å†…å­˜ï¼Œä»…ç”¨äºå…¼å®¹æ—§æ¥å£
    æ¨èç›´æ¥ä½¿ç”¨ build_dual_track_documents_generator è¿›è¡Œæµå¼å¤„ç†
    
    Returns:
        tuple: (rich_text_documents, basic_documents)
    """
    rich_text_documents = []
    basic_documents = []
    
    # ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼ï¼Œæµå¼å¤„ç†
    for doc in build_dual_track_documents_generator(db_path, llm_sum_file, filter_type="rich_text"):
        rich_text_documents.append(doc)
    
    for doc in build_dual_track_documents_generator(db_path, llm_sum_file, filter_type="basic"):
        basic_documents.append(doc)
    
    return rich_text_documents, basic_documents


def main():
    """ä¸»å‡½æ•°"""
    db_path = "data/database/final.db"
    
    if not Path(db_path).exists():
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return [], []
    
    rich_text_docs, basic_docs = build_dual_track_documents(db_path)
    
    return rich_text_docs, basic_docs


if __name__ == '__main__':
    rich_docs, basic_docs = main()
    print(f"\næ€»è®¡: {len(rich_docs) + len(basic_docs)} ä¸ªæ–‡æ¡£")

