#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸ºé—æ¼çš„ id ç”Ÿæˆæ‘˜è¦æ¡ç›®
"""

import json
import sqlite3
import asyncio
import time
from pathlib import Path
from typing import List, Dict

# å¯¼å…¥ 4_load_data.py ä¸­çš„å‡½æ•°
import sys
import importlib.util

# ä½¿ç”¨ importlib å¯¼å…¥ä»¥æ•°å­—å¼€å¤´çš„æ¨¡å—
spec = importlib.util.spec_from_file_location("load_data", "4_load_data.py")
load_data_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(load_data_module)

# å¯¼å…¥å¿…è¦çš„å‡½æ•°
setup_llm = load_data_module.setup_llm
load_series_with_episodes = load_data_module.load_series_with_episodes
generate_series_summary_async = load_data_module.generate_series_summary_async
extract_tags_from_profile = load_data_module.extract_tags_from_profile
save_progress = load_data_module.save_progress

# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆä¸ 4_load_data.py ç›¸åŒï¼‰
import os
if 'HF_ENDPOINT' not in os.environ:
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('ALL_PROXY', None)
os.environ.pop('all_proxy', None)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'


async def generate_summary_without_episodes(
    title: str,
    original_summary: str,
    llm
) -> tuple[str, str, dict]:
    """
    å³ä½¿æ²¡æœ‰ episodesï¼Œä¹Ÿä½¿ç”¨ LLM åŸºäºåŸå§‹ summary ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾
    
    Returns:
        tuple: (combined_text, plot_summary, metadata_dict)
    """
    from llama_index.core.response_synthesizers import TreeSummarize
    from llama_index.core import PromptTemplate
    
    if not original_summary or original_summary.strip() == '':
        return "æš‚æ— å‰§æƒ…ç®€ä»‹", "æš‚æ— å‰§æƒ…ç®€ä»‹", {}
    
    try:
        # å°†åŸå§‹ summary ä½œä¸ºæ–‡æœ¬å—
        text_chunks = [original_summary]
        
        # 1. ç”Ÿæˆå‰§æƒ…æ‘˜è¦
        summary_template_str = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†å‰§ç¼–è¾‘ã€‚ä»¥ä¸‹æ˜¯ç”µè§†å‰§ã€Š{title}ã€‹çš„å‰§æƒ…ç®€ä»‹ã€‚\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæ‰©å±•å¹¶ç²¾ç‚¼æˆä¸€æ®µæ›´è¯¦ç»†çš„å‰§æƒ…æ¦‚è¦ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. ä¿ç•™ä¸»çº¿æ•…äº‹è„‰ç»œï¼Œé€šè¿‡èµ·æ‰¿è½¬åˆæ¥æè¿°ã€‚\n"
            "2. åŒ…å«ä¸»è¦äººç‰©çš„å…³é”®è½¬æŠ˜ã€‚\n"
            "3. å­—æ•°æ§åˆ¶åœ¨ 500 å­—ä»¥å†…ã€‚\n"
            "4. è¯­è¨€æµç•…ï¼Œå¸å¼•è¯»è€…ã€‚\n"
            "ç”Ÿæˆçš„æ‘˜è¦ï¼š"
        )
        summary_template = PromptTemplate(summary_template_str)
        summarizer = TreeSummarize(
            llm=llm,
            summary_template=summary_template,
            verbose=False
        )
        query_str = f"è¯·ä¸ºç”µè§†å‰§ã€Š{title}ã€‹ç”Ÿæˆå‰§æƒ…æ‘˜è¦"
        plot_summary = await summarizer.aget_response(query_str=query_str, text_chunks=text_chunks)
        plot_summary = str(plot_summary).strip()
        
        # 2. ç”Ÿæˆäººç‰©ä¾§å†™
        character_profile_template_str = (
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†å‰§è§’è‰²åˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹å‰§æƒ…ç®€ä»‹ï¼Œè¯·åˆ†æç”µè§†å‰§ã€Š{title}ã€‹çš„æ ¸å¿ƒäººç‰©ä¸å…³ç³»ã€‚\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "è¯·**ä¸è¦**å¤è¿°å‰§æƒ…ï¼Œè€Œæ˜¯æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œè¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\n\n"
            "1. **æ ¸å¿ƒäººè®¾æ ‡ç­¾**ï¼š\n"
            "   - [è§’è‰²å]ï¼ˆä¸»è§’/é…è§’ï¼‰ï¼š\n"
            "     * æ€§æ ¼ç‰¹å¾ï¼š[æ€§æ ¼å½¢å®¹è¯1]ã€[æ€§æ ¼å½¢å®¹è¯2]ï¼ˆä¾‹å¦‚ï¼šè…¹é»‘ã€é˜³å…‰ã€é«˜å†·ã€ç¤¾æã€å‚²å¨‡ã€æ¸©æŸ”ã€ç†æ€§ã€æ„Ÿæ€§ï¼‰ã€‚\n"
            "     * èŒä¸šèº«ä»½ï¼š[å…·ä½“èŒä¸š]ï¼ˆä¾‹å¦‚ï¼šéœ¸é“æ€»è£ã€å¤–ç§‘åŒ»ç”Ÿã€å§åº•è­¦å¯Ÿã€è®¾è®¡å¸ˆã€å¾‹å¸ˆã€è®°è€…ã€æ•™å¸ˆã€ç¨‹åºå‘˜ã€æŠ•èµ„äººã€åŒ»ç”Ÿã€æŠ¤å£«ã€è­¦å¯Ÿã€å†›äººã€ç§‘å­¦å®¶ã€è‰ºæœ¯å®¶ã€å¨å¸ˆã€é£è¡Œå‘˜ç­‰ï¼‰ã€‚\n"
            "     * ç¤¾ä¼šåœ°ä½/èƒŒæ™¯ï¼šï¼ˆä¾‹å¦‚ï¼šå¯ŒäºŒä»£ã€è‰æ ¹é€†è¢­ã€ä¸–å®¶å­å¼Ÿã€æ™®é€šä¸Šç­æ—ï¼‰ã€‚\n"
            "2. **äººç‰©å…³ç³»æ¨¡å¼**ï¼š\n"
            "   - [è§’è‰²A] ä¸ [è§’è‰²B]ï¼š[å…³ç³»å½¢å®¹è¯]ï¼ˆä¾‹å¦‚ï¼šæ¬¢å–œå†¤å®¶ã€åŒå‘å¥”èµ´ã€ç›¸çˆ±ç›¸æ€ã€å…ˆå©šåçˆ±ã€é’æ¢…ç«¹é©¬ã€ä¸Šä¸‹çº§ã€å¸ˆç”Ÿã€åŒ»æ‚£ã€è­¦åŒªï¼‰ã€‚\n"
            "3. **çœ‹ç‚¹/é£æ ¼æ ‡ç­¾**ï¼š\n"
            "   - ï¼ˆä¾‹å¦‚ï¼šç”œå® ã€æ‚¬ç–‘çƒ§è„‘ã€èŒåœºé€†è¢­ã€æ²»æ„ˆã€è™æ‹æƒ…æ·±ã€ç ´é•œé‡åœ†ã€åŒ»ç–—ã€å¾‹æ”¿ã€å•†æˆ˜ã€æ ¡å›­ï¼‰ã€‚\n"
            "4. **æ ¸å¿ƒå†²çª/è®¾å®š**ï¼š\n"
            "   - ï¼ˆä¾‹å¦‚ï¼šèº«ä»½äº’æ¢ã€æ—¶ç©ºç©¿è¶Šã€å¤ä»‡ã€å•†æˆ˜ã€åŒ»ç–—æ•‘æ´ã€èŒåœºç«äº‰ã€å®¶æ—æ©æ€¨ï¼‰ã€‚\n\n"
            "**é‡è¦**ï¼šè¯·æ˜ç¡®æ ‡æ³¨ä¸»è§’çš„èŒä¸šèº«ä»½ï¼Œè¿™å¯¹ç”¨æˆ·æœç´¢éå¸¸é‡è¦ï¼ˆå¦‚ï¼šåŒ»ç”Ÿã€å¾‹å¸ˆã€æ€»è£ã€è­¦å¯Ÿç­‰ï¼‰ã€‚\n\n"
            "è¯·ç›´æ¥è¾“å‡ºåˆ†æç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜ï¼š"
        )
        character_profile_template = PromptTemplate(character_profile_template_str)
        char_summarizer = TreeSummarize(
            llm=llm,
            summary_template=character_profile_template,
            verbose=False
        )
        query_str = f"è¯·åˆ†æç”µè§†å‰§ã€Š{title}ã€‹çš„æ ¸å¿ƒäººç‰©ä¸å…³ç³»"
        character_profile = await char_summarizer.aget_response(query_str=query_str, text_chunks=text_chunks)
        character_profile = str(character_profile).strip()
        
        # 3. æå–æ ‡ç­¾
        metadata_dict = extract_tags_from_profile(character_profile, plot_summary)
        
        # 4. æ„å»º combined_text
        combined_text = (
            f"å‰§åï¼š{title}\n\n"
            f"=== äººç‰©ä¸çœ‹ç‚¹ ===\n{character_profile}\n\n"
            f"=== å‰§æƒ…æ¢—æ¦‚ ===\n{plot_summary}"
        )
        
        return combined_text, plot_summary, metadata_dict
        
    except Exception as e:
        error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
        error_type = type(e).__name__
        print(f"  ç”Ÿæˆæ‘˜è¦å¤±è´¥: [{error_type}] {error_msg}ï¼Œä½¿ç”¨åŸå§‹ç®€ä»‹")
        if original_summary:
            fallback = original_summary[:500]
            return fallback, fallback, {}
        return "æš‚æ— å‰§æƒ…ç®€ä»‹", "æš‚æ— å‰§æƒ…ç®€ä»‹", {}


async def generate_summary_for_missing_id(
    series: Dict,
    llm,
    use_key_episode_strategy: bool = True
) -> Dict:
    """
    ä¸ºå•ä¸ªé—æ¼çš„ id ç”Ÿæˆæ‘˜è¦
    
    Returns:
        dict: æ‘˜è¦æ•°æ®ï¼Œæ ¼å¼ä¸ summary_progress.json ä¸­çš„æ¡ç›®ç›¸åŒ
    """
    series_id = series['series_id']
    title = series['title']
    episodes = series.get('episodes', [])
    original_summary = series.get('summary', '') or ''
    
    print(f"  ğŸ”„ æ­£åœ¨ç”Ÿæˆ: ID={series_id} ã€Š{title}ã€‹ï¼ˆå…±{len(episodes)}é›†ï¼‰...")
    
    try:
        # å¦‚æœæœ‰ episodesï¼Œä½¿ç”¨æ­£å¸¸æµç¨‹
        if episodes:
            combined_text, plot_summary, metadata_dict = await generate_series_summary_async(
                title,
                original_summary,
                episodes,
                llm,
                max_length=500,
                use_key_episode_strategy=use_key_episode_strategy,
                use_dual_track=True
            )
        else:
            # å¦‚æœæ²¡æœ‰ episodesï¼Œä¹Ÿä½¿ç”¨ LLM åŸºäºåŸå§‹ summary ç”Ÿæˆ
            print(f"  âš  æ²¡æœ‰åˆ†é›†æ•°æ®ï¼Œå°†åŸºäºåŸå§‹ç®€ä»‹ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦...")
            combined_text, plot_summary, metadata_dict = await generate_summary_without_episodes(
                title,
                original_summary,
                llm
            )
        
        # æ„å»ºä¸ summary_progress.json ç›¸åŒæ ¼å¼çš„æ¡ç›®
        summary_entry = {
            "series_id": series_id,
            "title": title,
            "plot_summary": plot_summary,
            "combined_text": combined_text,
            "tags": metadata_dict.get('tags', []),
            "occupation_tags": metadata_dict.get('occupation_tags', []),
            "character_tags": metadata_dict.get('character_tags', []),
            "style_tags": metadata_dict.get('style_tags', []),
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"  âœ“ ç”Ÿæˆå®Œæˆ: ID={series_id} ã€Š{title}ã€‹")
        return summary_entry
        
    except Exception as e:
        error_msg = str(e) if e else "æœªçŸ¥é”™è¯¯"
        error_type = type(e).__name__
        print(f"  âŒ ç”Ÿæˆå¤±è´¥: ID={series_id} ã€Š{title}ã€‹- [{error_type}] {error_msg}")
        
        # å¤±è´¥æ—¶ä½¿ç”¨åŸå§‹æ‘˜è¦
        fallback_summary = original_summary or 'æš‚æ— å‰§æƒ…ç®€ä»‹'
        return {
            "series_id": series_id,
            "title": title,
            "plot_summary": fallback_summary,
            "combined_text": fallback_summary,
            "tags": [],
            "occupation_tags": [],
            "character_tags": [],
            "style_tags": [],
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "error": f"{error_type}: {error_msg}"
        }


def load_missing_ids() -> List[int]:
    """åŠ è½½é—æ¼çš„ id åˆ—è¡¨"""
    missing_ids_file = Path("missing_ids.json")
    
    if not missing_ids_file.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ° {missing_ids_file}")
        return []
    
    with open(missing_ids_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data.get('missing_ids', [])


def load_series_by_ids(db_path: str, series_ids: List[int]) -> List[Dict]:
    """
    ä»æ•°æ®åº“åŠ è½½æŒ‡å®š id çš„å‰§é›†æ•°æ®
    
    Args:
        db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        series_ids: è¦åŠ è½½çš„ series_id åˆ—è¡¨
    
    Returns:
        List[Dict]: å‰§é›†æ•°æ®åˆ—è¡¨
    """
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    # æ„å»º IN æŸ¥è¯¢
    placeholders = ','.join(['?'] * len(series_ids))
    
    cursor.execute(f'''
        SELECT 
            s.id as series_id,
            s.original_doc_id,
            s.title,
            s.summary,
            s.cast,
            s.director,
            s.year,
            s.region,
            s.genre,
            s.url as series_url,
            e.id as episode_id,
            e.ep_number,
            e.episode_title,
            e.content as episode_content,
            e.episode_url
        FROM series s
        LEFT JOIN episodes e ON s.id = e.series_id
        WHERE s.id IN ({placeholders})
        ORDER BY s.id, e.ep_number
    ''', series_ids)
    
    # æŒ‰series_idåˆ†ç»„
    series_dict = {}
    for row in cursor.fetchall():
        series_id = row['series_id']
        
        if series_id not in series_dict:
            series_dict[series_id] = {
                'series_id': series_id,
                'original_doc_id': row['original_doc_id'],
                'title': row['title'],
                'summary': row['summary'] or '',
                'cast': row['cast'] or '',
                'director': row['director'] or '',
                'year': row['year'] or '',
                'region': row['region'] or '',
                'genre': row['genre'] or '',
                'url': row['series_url'],
                'episodes': []
            }
        
        # æ·»åŠ åˆ†é›†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if row['episode_id']:
            series_dict[series_id]['episodes'].append({
                'ep_number': row['ep_number'],
                'episode_title': row['episode_title'] or '',
                'content': row['episode_content'] or '',
                'episode_url': row['episode_url'] or ''
            })
    
    conn.close()
    
    # æŒ‰ series_ids çš„é¡ºåºè¿”å›
    result = []
    for sid in series_ids:
        if sid in series_dict:
            result.append(series_dict[sid])
        else:
            print(f"  âš  è­¦å‘Š: æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ° id={sid} çš„å‰§é›†")
    
    return result


async def generate_all_missing_summaries(
    missing_ids: List[int],
    db_path: str = "data/database/final.db",
    output_file: str = "missing_summaries.json",
    max_workers: int = 3,
    use_key_episode_strategy: bool = True
):
    """
    ä¸ºæ‰€æœ‰é—æ¼çš„ id ç”Ÿæˆæ‘˜è¦
    
    Args:
        missing_ids: é—æ¼çš„ id åˆ—è¡¨
        db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_workers: æœ€å¤§å¹¶å‘æ•°
        use_key_episode_strategy: æ˜¯å¦ä½¿ç”¨å…³é”®é›†åŠ æƒæ³•
    """
    print(f"å¼€å§‹ä¸º {len(missing_ids)} ä¸ªé—æ¼çš„ id ç”Ÿæˆæ‘˜è¦...")
    
    # åŠ è½½ LLM
    print("\nåˆå§‹åŒ– LLM...")
    llm = setup_llm()
    print("âœ“ LLM åˆå§‹åŒ–å®Œæˆ")
    
    # ä»æ•°æ®åº“åŠ è½½æ•°æ®
    print(f"\nä»æ•°æ®åº“åŠ è½½æ•°æ® (db_path={db_path})...")
    series_data = load_series_by_ids(db_path, missing_ids)
    print(f"âœ“ åŠ è½½äº† {len(series_data)} ä¸ªå‰§é›†çš„æ•°æ®")
    
    if not series_data:
        print("âš  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œé€€å‡º")
        return
    
    # ç”Ÿæˆæ‘˜è¦ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
    print(f"\nå¼€å§‹ç”Ÿæˆæ‘˜è¦ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰...")
    
    semaphore = asyncio.Semaphore(max_workers)
    
    async def process_with_semaphore(series):
        async with semaphore:
            return await generate_summary_for_missing_id(
                series, llm, use_key_episode_strategy
            )
    
    # åˆ›å»ºä»»åŠ¡
    tasks = [process_with_semaphore(s) for s in series_data]
    
    # æ”¶é›†ç»“æœï¼ˆä½¿ç”¨ as_completed ä»¥ä¾¿å®æ—¶ä¿å­˜ï¼‰
    summaries = {}
    completed_count = 0
    total_count = len(series_data)
    
    # ä½¿ç”¨ asyncio.as_completed æ¥å®æ—¶å¤„ç†å®Œæˆçš„ä»»åŠ¡
    for coro in asyncio.as_completed(tasks):
        try:
            result = await coro
            completed_count += 1
            
            if isinstance(result, Exception):
                # å¦‚æœç»“æœæ˜¯å¼‚å¸¸ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥çŸ¥é“æ˜¯å“ªä¸ª series_id
                # ä½†å¯ä»¥é€šè¿‡æ£€æŸ¥ summaries ä¸­ç¼ºå°‘çš„ id æ¥æ¨æ–­
                processed_ids = {int(k) for k in summaries.keys()}
                all_ids = {s['series_id'] for s in series_data}
                missing_ids = all_ids - processed_ids
                if missing_ids:
                    series_id = min(missing_ids)  # å–ç¬¬ä¸€ä¸ªç¼ºå¤±çš„
                    # æ‰¾åˆ°å¯¹åº”çš„ series
                    series = next((s for s in series_data if s['series_id'] == series_id), None)
                    if series:
                        print(f"  âŒ å¤„ç†å¤±è´¥: ID={series_id} - {result}")
                        summaries[str(series_id)] = {
                            "series_id": series_id,
                            "title": series.get('title', 'æœªçŸ¥'),
                            "plot_summary": "ç”Ÿæˆå¤±è´¥",
                            "combined_text": "ç”Ÿæˆå¤±è´¥",
                            "tags": [],
                            "occupation_tags": [],
                            "character_tags": [],
                            "style_tags": [],
                            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "error": str(result)
                        }
                else:
                    print(f"  âŒ å¤„ç†å¤±è´¥ï¼Œä½†æ— æ³•ç¡®å®š series_id - {result}")
            else:
                summaries[str(result['series_id'])] = result
                print(f"  âœ“ [{completed_count}/{total_count}] å®Œæˆ: ID={result['series_id']} ã€Š{result['title']}ã€‹")
            
            # æ¯å®Œæˆ 1 ä¸ªå°±ä¿å­˜ä¸€æ¬¡ï¼ˆç¬¬ä¸€ä¸ªç«‹å³ä¿å­˜ï¼Œä¹‹åæ¯ 5 ä¸ªä¿å­˜ä¸€æ¬¡ï¼‰
            if completed_count == 1 or completed_count % 5 == 0:
                print(f"  ğŸ’¾ ä¿å­˜è¿›åº¦ ({completed_count}/{total_count})...")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(summaries, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            completed_count += 1
            print(f"  âŒ å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
    
    # æœ€ç»ˆä¿å­˜åˆ°æ–‡ä»¶
    print(f"\nä¿å­˜ç»“æœåˆ° {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ å®Œæˆï¼å…±ç”Ÿæˆ {len(summaries)} ä¸ªæ‘˜è¦ï¼Œå·²ä¿å­˜åˆ° {output_file}")
    
    # ç»Ÿè®¡
    success_count = sum(1 for s in summaries.values() if 'error' not in s)
    error_count = len(summaries) - success_count
    print(f"\nç»Ÿè®¡:")
    print(f"  æˆåŠŸ: {success_count} ä¸ª")
    print(f"  å¤±è´¥: {error_count} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é—æ¼çš„ id
    print("=" * 60)
    print("ä¸ºé—æ¼çš„ id ç”Ÿæˆæ‘˜è¦")
    print("=" * 60)
    
    missing_ids = load_missing_ids()
    
    if not missing_ids:
        print("æ²¡æœ‰æ‰¾åˆ°é—æ¼çš„ idï¼Œé€€å‡º")
        return
    
    print(f"æ‰¾åˆ° {len(missing_ids)} ä¸ªé—æ¼çš„ id")
    print(f"é—æ¼çš„ id: {missing_ids[:20]}{'...' if len(missing_ids) > 20 else ''}")
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
    db_path = Path("data/database/final.db")
    if not db_path.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶ {db_path}")
        return
    
    # ç”Ÿæˆæ‘˜è¦
    asyncio.run(generate_all_missing_summaries(
        missing_ids,
        db_path=str(db_path),
        output_file="missing_summaries.json",
        max_workers=3,
        use_key_episode_strategy=True
    ))


if __name__ == "__main__":
    main()

